{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"llama-cpp-chat-memory This project is intended as an example and a basic framework for a locally run chatbot with documents. The target user group is developers with some understanding about python and llm framworks. If you want to learn about llm and AI, when you can take a look at my llm resources for beginners or PygWiki . This project is mainly intended to serve as a more fleshed out tutorial and a basic frame to test various things like document embeddings. For this reason, the chatbot itself is intended to be lightweight and simple. You can also use this chatbot to test models and prompts. The document fetching can be disabled by setting collection to \"\" in the config files. This leaves you with just a basic character chatbot. Everything is designed to run locally. The model is run with llama.cpp and it's python bindings, the UI is Chainlit, the vector database is Chroma and everythin is glued together with Langchain. Document processing uses Spacy and Sentence Transformers and Playwright. There are no dependencies to external api's. Llama.cpp can use gpu acceleration with Cuda and Blas. See the documentation for llama-cpp-python for documentation. The chatbot uses character cards as prompts. The supported cards are Tavern and V2. Internal lorebooks are not supported yet. There are several scripts for parsing json lorebooks, pdt, textfiles and scarping web pages for the memory content. Also included are scripts for parsing metadata from documents automatically.","title":"Home"},{"location":"#llama-cpp-chat-memory","text":"This project is intended as an example and a basic framework for a locally run chatbot with documents. The target user group is developers with some understanding about python and llm framworks. If you want to learn about llm and AI, when you can take a look at my llm resources for beginners or PygWiki . This project is mainly intended to serve as a more fleshed out tutorial and a basic frame to test various things like document embeddings. For this reason, the chatbot itself is intended to be lightweight and simple. You can also use this chatbot to test models and prompts. The document fetching can be disabled by setting collection to \"\" in the config files. This leaves you with just a basic character chatbot. Everything is designed to run locally. The model is run with llama.cpp and it's python bindings, the UI is Chainlit, the vector database is Chroma and everythin is glued together with Langchain. Document processing uses Spacy and Sentence Transformers and Playwright. There are no dependencies to external api's. Llama.cpp can use gpu acceleration with Cuda and Blas. See the documentation for llama-cpp-python for documentation. The chatbot uses character cards as prompts. The supported cards are Tavern and V2. Internal lorebooks are not supported yet. There are several scripts for parsing json lorebooks, pdt, textfiles and scarping web pages for the memory content. Also included are scripts for parsing metadata from documents automatically.","title":"llama-cpp-chat-memory"},{"location":"UNLICENSE/","text":"This is free and unencumbered software released into the public domain. Anyone is free to copy, modify, publish, use, compile, sell, or distribute this software, either in source code form or as a compiled binary, for any purpose, commercial or non-commercial, and by any means. In jurisdictions that recognize copyright laws, the author or authors of this software dedicate any and all copyright interest in the software to the public domain. We make this dedication for the benefit of the public at large and to the detriment of our heirs and successors. We intend this dedication to be an overt act of relinquishment in perpetuity of all present and future rights to this software under copyright law. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. For more information, please refer to https://unlicense.org","title":"License"},{"location":"card_format/","text":"Card Format See character editor . There are few example cards included like'Skynet', 'Harry Potter' and 'Bronya Zaychik' 'name' : 'char_name' The name for the ai character. When using json or yaml, this is expected to correspond to avatar image. name.png or name.jpg. 'description' : 'char_persona' The description for the character personality. Likes, dislikes, personality traits. 'scenario' : 'world_scenario' Description of the scenario. This roughly corresponds to things like \"You are a hr customer service having a discussion with a customer. Always be polite\". etc. 'mes_example' : 'example_dialogue' Example dialogue. The AI will pick answer patterns based on this 'first_mes' : 'char_greeting' A landing page for the chat. This will not be included in the prompt. The documents folder includes some documents for embeddings parsing for the character cards.","title":"Card Format"},{"location":"card_format/#card-format","text":"See character editor . There are few example cards included like'Skynet', 'Harry Potter' and 'Bronya Zaychik' 'name' : 'char_name' The name for the ai character. When using json or yaml, this is expected to correspond to avatar image. name.png or name.jpg. 'description' : 'char_persona' The description for the character personality. Likes, dislikes, personality traits. 'scenario' : 'world_scenario' Description of the scenario. This roughly corresponds to things like \"You are a hr customer service having a discussion with a customer. Always be polite\". etc. 'mes_example' : 'example_dialogue' Example dialogue. The AI will pick answer patterns based on this 'first_mes' : 'char_greeting' A landing page for the chat. This will not be included in the prompt. The documents folder includes some documents for embeddings parsing for the character cards.","title":"Card Format"},{"location":"configs/","text":"Basic Configs You can change the configuration settings in .env file. The available embeddings are llama,spacy and hugginface. Make sure that the config for the chat matches the embeddings that were used to create the chroma collection. VECTOR_K is the value for vector storage documents for how many documents should be returned. You might need to change this based on your context and vector store chunk size. BUFFER_K is the size for conversation buffer. The prompt will include last K qustion answer pairs. Having large VECTOR_K and BUFFER_K can overfill the prompt. The default character card is Skynet_V2.png. This is just a basic template. Config Field Description MODEL_DIR The dir for the models MODEL model_name.gguf MODEL_TYPE alpaca/mistral LAYERS Number of layers to offload to gpu CHARACTER_CARD_DIR The directory for chracter cards CHARACTER_CARD character_card.png/yaml/json PERSIST_DIRECTORY dir for chroma embeddings PROMPT_TEMPLATE_DIRECTORY Prompt template are stored here PROMPT_TEMPLATE question_generation_template.json REPLACE_YOU Replace references to \"You\" in card with \"User\" KEY_STORAGE_DIRECTORY dir for NER keys for chroma USE_KEY_STORAGE Use NER keys for Chroma metadata COLLECTION Chroma collection to use. \"\" to disable Chroma QUERY_TYPE Embeddings type. \"mmr\" or \"similarity\" EMBEDDINGS_TYPE llama/spacy/hugginface EMBEDDINGS_model spacy/hugginface model name (needs to be installed) FETCH_K Fetch k closest embeddings for similiarity LAMBDA_MULT Lambda for Chroma VECTOR_K Fetch k closest embeddings for mmr BUFFER_K Buffer last k exchanges to conversation context ROPE_CONTEXT Rope context for rope scaling N_CTX Context size USE_MAX_TOKENS Use max tokens. True/False MAX_TOKENS Max tokens General Configs Other configs are found in the run_files folder. These include Webscrape configs, ner parse configs and filter configs. Filters folder defines the general webscrape filters to clean the documents. This file uses regex and can easily be modified to add extra filtering. Parse_configs defines the expected csv column structure and ner type parsing. This includes noun engrams, entities, noun chunks and parse type. Web scrape configs define the web pages fo a scrape. This is convinient if you want to scrape multiple pages.","title":"Configs"},{"location":"configs/#basic-configs","text":"You can change the configuration settings in .env file. The available embeddings are llama,spacy and hugginface. Make sure that the config for the chat matches the embeddings that were used to create the chroma collection. VECTOR_K is the value for vector storage documents for how many documents should be returned. You might need to change this based on your context and vector store chunk size. BUFFER_K is the size for conversation buffer. The prompt will include last K qustion answer pairs. Having large VECTOR_K and BUFFER_K can overfill the prompt. The default character card is Skynet_V2.png. This is just a basic template. Config Field Description MODEL_DIR The dir for the models MODEL model_name.gguf MODEL_TYPE alpaca/mistral LAYERS Number of layers to offload to gpu CHARACTER_CARD_DIR The directory for chracter cards CHARACTER_CARD character_card.png/yaml/json PERSIST_DIRECTORY dir for chroma embeddings PROMPT_TEMPLATE_DIRECTORY Prompt template are stored here PROMPT_TEMPLATE question_generation_template.json REPLACE_YOU Replace references to \"You\" in card with \"User\" KEY_STORAGE_DIRECTORY dir for NER keys for chroma USE_KEY_STORAGE Use NER keys for Chroma metadata COLLECTION Chroma collection to use. \"\" to disable Chroma QUERY_TYPE Embeddings type. \"mmr\" or \"similarity\" EMBEDDINGS_TYPE llama/spacy/hugginface EMBEDDINGS_model spacy/hugginface model name (needs to be installed) FETCH_K Fetch k closest embeddings for similiarity LAMBDA_MULT Lambda for Chroma VECTOR_K Fetch k closest embeddings for mmr BUFFER_K Buffer last k exchanges to conversation context ROPE_CONTEXT Rope context for rope scaling N_CTX Context size USE_MAX_TOKENS Use max tokens. True/False MAX_TOKENS Max tokens","title":"Basic Configs"},{"location":"configs/#general-configs","text":"Other configs are found in the run_files folder. These include Webscrape configs, ner parse configs and filter configs. Filters folder defines the general webscrape filters to clean the documents. This file uses regex and can easily be modified to add extra filtering. Parse_configs defines the expected csv column structure and ner type parsing. This includes noun engrams, entities, noun chunks and parse type. Web scrape configs define the web pages fo a scrape. This is convinient if you want to scrape multiple pages.","title":"General Configs"},{"location":"creating_embeddings/","text":"Creating embeddings The embeddings creation uses env setting for threading and cuda. The Example documents are in the Documents folder. The scripts are in the documents_parsing folder. Use --help for basic instructions. The parsing script will parse all txt, pdf or json files in the target directory. For json lorebooks a key_storage file will also be created for metadata filtering. You need to download models for NER parsing. Textacy parses text files with Spacy sentence transformers to automatically generate keys for metadata filters. The default model is en_core_web_lg. See available models at Spacy Models python -m spacy download en_core_web_sm python -m spacy download en_core_web_md python -m spacy download en_core_web_lg You might want to play with the chunk size and overlap based on your text documents The example documents include a txt file for skynet embeddings and json lorebooks for Hogwarts and Honkai Impact The supported lorebook formats are chub inferred AgnAIstic and SillyTavern original source. For pdf files there is a pdf file of short stories from Fyodor Dostoyevsky included The source is Internet Archive, the copy is in public domain. The pdf text quality is quite poor thought, so I recommend getting another file, !!Important!!. You need to make sure that the documents, character_storage and key_storage folders exist. Textacy parsing will use NER to parse keys from the document using sentence transformers. This keys can be used as Chroma metadata, NOTE: Textacy parsing will create a key file in key_storage that can be used by text parsing. Json files will create keys automatically if present in json file. python -m document_parsing.textacy_parsing --collection-name skynet --embeddings-type spacy Parse csv to text python -m document_parsing.parse_csv_to_text Parse the documents with python -m document_parsing.parse_text_documents python -m document_parsing.parse_text_documents python -m document_parsing.parse_json_documents You can test the embeddings with python -m document_parsing.test_embeddings --collection-name skynet --query \"Who is John Connor\" --embeddings-type llama python -m document_parsing.test_embeddings --collection-name skynet2 --query \"Who is John Connor\" --embeddings-type spacy python -m document_parsing.test_embeddings --collection-name hogwarts --query \"Who is Charles Rookwood'\" --embeddings-type spacy Optional params Description --data-directory The directory where your text files are stored. Default \"./run_files/documents/skynet\" --collection-name The name of the collection. Default \"skynet\" --persist-directory The directory where you want to store the Chroma collection. Default \"./run_files/character_storage/\" --key-storage The directory for the collection metadata keys Need to be created with textacy parsing. Default \"./run_files/key_storage/\" --chunk-size The text chunk size for parsing. Default \"1024\" --chunk-overlap The overlap for text chunks for parsing. Default \"0\" --embeddings-type The chosen embeddings type. Default \"spacy\"","title":"Creating embeddings"},{"location":"creating_embeddings/#creating-embeddings","text":"The embeddings creation uses env setting for threading and cuda. The Example documents are in the Documents folder. The scripts are in the documents_parsing folder. Use --help for basic instructions. The parsing script will parse all txt, pdf or json files in the target directory. For json lorebooks a key_storage file will also be created for metadata filtering. You need to download models for NER parsing. Textacy parses text files with Spacy sentence transformers to automatically generate keys for metadata filters. The default model is en_core_web_lg. See available models at Spacy Models python -m spacy download en_core_web_sm python -m spacy download en_core_web_md python -m spacy download en_core_web_lg You might want to play with the chunk size and overlap based on your text documents The example documents include a txt file for skynet embeddings and json lorebooks for Hogwarts and Honkai Impact The supported lorebook formats are chub inferred AgnAIstic and SillyTavern original source. For pdf files there is a pdf file of short stories from Fyodor Dostoyevsky included The source is Internet Archive, the copy is in public domain. The pdf text quality is quite poor thought, so I recommend getting another file, !!Important!!. You need to make sure that the documents, character_storage and key_storage folders exist. Textacy parsing will use NER to parse keys from the document using sentence transformers. This keys can be used as Chroma metadata, NOTE: Textacy parsing will create a key file in key_storage that can be used by text parsing. Json files will create keys automatically if present in json file. python -m document_parsing.textacy_parsing --collection-name skynet --embeddings-type spacy Parse csv to text python -m document_parsing.parse_csv_to_text Parse the documents with python -m document_parsing.parse_text_documents python -m document_parsing.parse_text_documents python -m document_parsing.parse_json_documents You can test the embeddings with python -m document_parsing.test_embeddings --collection-name skynet --query \"Who is John Connor\" --embeddings-type llama python -m document_parsing.test_embeddings --collection-name skynet2 --query \"Who is John Connor\" --embeddings-type spacy python -m document_parsing.test_embeddings --collection-name hogwarts --query \"Who is Charles Rookwood'\" --embeddings-type spacy Optional params Description --data-directory The directory where your text files are stored. Default \"./run_files/documents/skynet\" --collection-name The name of the collection. Default \"skynet\" --persist-directory The directory where you want to store the Chroma collection. Default \"./run_files/character_storage/\" --key-storage The directory for the collection metadata keys Need to be created with textacy parsing. Default \"./run_files/key_storage/\" --chunk-size The text chunk size for parsing. Default \"1024\" --chunk-overlap The overlap for text chunks for parsing. Default \"0\" --embeddings-type The chosen embeddings type. Default \"spacy\"","title":"Creating embeddings"},{"location":"examples/","text":"Some examples","title":"Some Examples"},{"location":"examples/#some-examples","text":"","title":"Some examples"},{"location":"getting_started/","text":"You will need hatch to run this project. You can install hatch with pipx. See Hatch and Pipx . The commands here are for windows powershell. If you use another shell, you'll have to change things as needed. pip install pipx pipx install hatch Then from the repo root folder run. hatch shell chat cd .\\src\\llama_cpp_chat_memory\\ python -m spacy download en_core_web_lg playwright install You will need spacy models for text embeddings if you do not use llama-cpp embeddings. Playwright is used by the old webscrape scripts. These are not needed for running the chatbot itself. You also might want to run llama-cpp with gpu acceleration like cuda. See llama-cpp-python for specifics. Then run: $env:FORCE_CMAKE=1 $env:CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --no-deps Note that this example is for powershell and for the latest llama-cpp-python. You will need to change the command based on the terminal and the llama-cpp-python version. Get a gguf model from a site like The Bloke and a character card and lorebooks from a site like Chub.ai or make your own with character editor Change the .env_test to .env and make sure that the correct folders exist. You can set the collection to \"\" and try the chatbot by running: chainlit run character_chat.py If you want to create memory then see more details below.","title":"Getting started"},{"location":"named_entity_recognition/","text":"Named Entity Recognition(NER) You can use textacy_parsing script for generating document metadata keys automatically. The scripts are a modified version of textacy code updated to run with the current spacy version. The script uses a spacy embeddings model to process a text document for a json metadata keyfile. The keys are parsed based on a config file in run_files/parse_configs/ner_types.json or run_files/parse_configs/ner_types_full.json. You can give your own config file if you want. The available configs are Ngrams Description PROPN Proper Noun NOUN Noun ADJ Adjective NNP Noun proper singular NN Noun, singular or mass AUX Auxiliary VBZ Verb, 3rd person singular present VERB Verb ADP Adposition SYM Symbol NUM Numeral CD Cardinal number VBG verb, gerund or present participle ROOT Root Entities Description FAC Buildings, airports, highways, bridges, etc. NORP Nationalities or religious or political groups GPE Countries, cities, states PRODUCT Objects, vehicles, foods, etc. (not services) EVENT Named hurricanes, battles, wars, sports events, etc. PERSON People, including fictional ORG Companies, agencies, institutions, etc. LOC Non-GPE locations, mountain ranges, bodies of water DATE Absolute or relative dates or periods TIME Times smaller than a day WORK_OF_ART Titles of books, songs, etc. Extract type Description orth Terms are represented by their text exactly as written lower Lowercased form of the text lemma Base form w/o inflectional suffixes For details see Spacy linguistic features and Model NER labels . The instructions expect en model, but spacy supports a wide range of models. You can also specify Noun chunks. Noun chunk of 2 for example would create keys like \"Yellow House\" or \"Blond Hair\". You can create ner metadata list with python -m document_parsing.parse_ner Optional param Description --data-directory The directory where your text files are stored. Default \"./run_files/documents/skynet\" --collection-name The name of the collection Will be used as name and location for the keyfile. Default \"skynet\" --key-storage The directory for the collection metadata keys. Default \"./run_files/key_storage/\"","title":"Named Entity Recognition(NER)"},{"location":"named_entity_recognition/#named-entity-recognitionner","text":"You can use textacy_parsing script for generating document metadata keys automatically. The scripts are a modified version of textacy code updated to run with the current spacy version. The script uses a spacy embeddings model to process a text document for a json metadata keyfile. The keys are parsed based on a config file in run_files/parse_configs/ner_types.json or run_files/parse_configs/ner_types_full.json. You can give your own config file if you want. The available configs are Ngrams Description PROPN Proper Noun NOUN Noun ADJ Adjective NNP Noun proper singular NN Noun, singular or mass AUX Auxiliary VBZ Verb, 3rd person singular present VERB Verb ADP Adposition SYM Symbol NUM Numeral CD Cardinal number VBG verb, gerund or present participle ROOT Root Entities Description FAC Buildings, airports, highways, bridges, etc. NORP Nationalities or religious or political groups GPE Countries, cities, states PRODUCT Objects, vehicles, foods, etc. (not services) EVENT Named hurricanes, battles, wars, sports events, etc. PERSON People, including fictional ORG Companies, agencies, institutions, etc. LOC Non-GPE locations, mountain ranges, bodies of water DATE Absolute or relative dates or periods TIME Times smaller than a day WORK_OF_ART Titles of books, songs, etc. Extract type Description orth Terms are represented by their text exactly as written lower Lowercased form of the text lemma Base form w/o inflectional suffixes For details see Spacy linguistic features and Model NER labels . The instructions expect en model, but spacy supports a wide range of models. You can also specify Noun chunks. Noun chunk of 2 for example would create keys like \"Yellow House\" or \"Blond Hair\". You can create ner metadata list with python -m document_parsing.parse_ner Optional param Description --data-directory The directory where your text files are stored. Default \"./run_files/documents/skynet\" --collection-name The name of the collection Will be used as name and location for the keyfile. Default \"skynet\" --key-storage The directory for the collection metadata keys. Default \"./run_files/key_storage/\"","title":"Named Entity Recognition(NER)"},{"location":"preparing_the_env/","text":"Preparing the env You will need a llama model that is compatible with llama-cpp. See models in HuggingFace by The Bloke You might want to build with cuda support. You need to pass FORCE_CMAKE=1 and CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" to env variables. This is the powershell syntax. Use whatever syntax your shell uses to set env variables You need to download language models if you use NER parsing, embeddings or spacy sentence transformers. The default model is en_core_web_lg. See available models at Spacy Models Choose the preferred model size and type. python -m spacy download en_core_web_sm python -m spacy download en_core_web_md python -m spacy download en_core_web_lg For installing dependencies in the virtual envs with hatch hatch env create Copy the .env_test to .env and set directories and model settings NOTE: Setting collection to \"\" will disable chroma fetching and you will get a normal character chatbot.","title":"Preparing the env"},{"location":"preparing_the_env/#preparing-the-env","text":"You will need a llama model that is compatible with llama-cpp. See models in HuggingFace by The Bloke You might want to build with cuda support. You need to pass FORCE_CMAKE=1 and CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" to env variables. This is the powershell syntax. Use whatever syntax your shell uses to set env variables You need to download language models if you use NER parsing, embeddings or spacy sentence transformers. The default model is en_core_web_lg. See available models at Spacy Models Choose the preferred model size and type. python -m spacy download en_core_web_sm python -m spacy download en_core_web_md python -m spacy download en_core_web_lg For installing dependencies in the virtual envs with hatch hatch env create Copy the .env_test to .env and set directories and model settings NOTE: Setting collection to \"\" will disable chroma fetching and you will get a normal character chatbot.","title":"Preparing the env"},{"location":"prompt_support/","text":"Prompt Support Supports alpaca and mistral text prompts, V2 and tavern style json and yaml files and V2 and tavern png cards. Avatar images need to be in the same folder as the prompt file. V2 and Tavern png files get a copy of the image without exif data in the project temp file. Inbuilt lorebooks are currently not supported See Chub.ai for some character cards or make your own with character editor .","title":"Prompt Support"},{"location":"prompt_support/#prompt-support","text":"Supports alpaca and mistral text prompts, V2 and tavern style json and yaml files and V2 and tavern png cards. Avatar images need to be in the same folder as the prompt file. V2 and Tavern png files get a copy of the image without exif data in the project temp file. Inbuilt lorebooks are currently not supported See Chub.ai for some character cards or make your own with character editor .","title":"Prompt Support"},{"location":"running_the_chatbot/","text":"Running the chatbot To run the chatbot. cd src\\llama_cpp_langchain_chat chainlit run character_chat.py The chatbot should open in your browser","title":"Running the chatbot"},{"location":"running_the_chatbot/#running-the-chatbot","text":"To run the chatbot. cd src\\llama_cpp_langchain_chat chainlit run character_chat.py The chatbot should open in your browser","title":"Running the chatbot"},{"location":"running_the_env/","text":"Running the env You'll need to run all the commands inside the virtual env. hatch shell chat (optional for cuda support)$env:FORCE_CMAKE=1 (optional for cuda support)$env:CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" (optional for cuda support)pip install llama-cpp-python==VERSION --force-reinstall --upgrade --no-cache-dir --no-deps cd src\\llama_cpp_langchain_chat","title":"Running the env"},{"location":"running_the_env/#running-the-env","text":"You'll need to run all the commands inside the virtual env. hatch shell chat (optional for cuda support)$env:FORCE_CMAKE=1 (optional for cuda support)$env:CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" (optional for cuda support)pip install llama-cpp-python==VERSION --force-reinstall --upgrade --no-cache-dir --no-deps cd src\\llama_cpp_langchain_chat","title":"Running the env"},{"location":"webscraping/","text":"Webscraping You can scrape web pages to text documents in order to use them as documents for chroma. Optional. The old web scraping uses playwright and requires that the web engines are installed. After starting the virtual env run: playwright install The web scraping is prepared with config files in web_scrape_configs folder. The format is in json. See the example files for the specfics. A number of regex filters are used to clean the scrape data. You can modify and add filters if you want. The filters are stored in the src/llama_cpp_chat_memory/run_files/filters/web_scrape_filters.json file. To run the scrape run: python -m document_parsing.web_scraper</BR> Optional param Description --data-directory The directory where your text files are stored. Default \"./run_files/documents/skynet\" --collection-name The name of the collection. Default \"skynet\" --web-scrape-directory The config file to be used for the webscrape. Default \"./run_files/web_scrape_configs/\"","title":"Webscraping"},{"location":"webscraping/#webscraping","text":"You can scrape web pages to text documents in order to use them as documents for chroma. Optional. The old web scraping uses playwright and requires that the web engines are installed. After starting the virtual env run: playwright install The web scraping is prepared with config files in web_scrape_configs folder. The format is in json. See the example files for the specfics. A number of regex filters are used to clean the scrape data. You can modify and add filters if you want. The filters are stored in the src/llama_cpp_chat_memory/run_files/filters/web_scrape_filters.json file. To run the scrape run: python -m document_parsing.web_scraper</BR> Optional param Description --data-directory The directory where your text files are stored. Default \"./run_files/documents/skynet\" --collection-name The name of the collection. Default \"skynet\" --web-scrape-directory The config file to be used for the webscrape. Default \"./run_files/web_scrape_configs/\"","title":"Webscraping"}]}