{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"llama-cpp-chat-memory This project is intended as an example and a basic framework for a locally run chatbot with documents. The target user group is developers with some understanding about python and llm framworks. If you want to learn about llm and AI, when you can take a look at my llm resources for beginners or PygWiki . This project is mainly intended to serve as a more fleshed out tutorial and a basic frame to test various things like document embeddings. For this reason, the chatbot itself is intended to be lightweight and simple. You can also use this chatbot to test models and prompts. The document fetching can be disabled by setting collection to \"\" in the config files. This leaves you with just a basic character chatbot. Everything is designed to run locally. The model is run with llama.cpp and it's python bindings, the UI is Chainlit, the vector database is Chroma and everythin is glued together with Langchain. Document processing uses Spacy and Sentence Transformers and Playwright. There are no dependencies to external api's. Llama.cpp can use gpu acceleration with Cuda and Blas. See the documentation for llama-cpp-python for documentation. The chatbot uses character cards as prompts. The supported cards are Tavern and V2. Internal lorebooks are not supported yet. There are several scripts for parsing json lorebooks, pdt, textfiles and scarping web pages for the memory content. Also included are scripts for parsing metadata from documents automatically.","title":"Home"},{"location":"#llama-cpp-chat-memory","text":"This project is intended as an example and a basic framework for a locally run chatbot with documents. The target user group is developers with some understanding about python and llm framworks. If you want to learn about llm and AI, when you can take a look at my llm resources for beginners or PygWiki . This project is mainly intended to serve as a more fleshed out tutorial and a basic frame to test various things like document embeddings. For this reason, the chatbot itself is intended to be lightweight and simple. You can also use this chatbot to test models and prompts. The document fetching can be disabled by setting collection to \"\" in the config files. This leaves you with just a basic character chatbot. Everything is designed to run locally. The model is run with llama.cpp and it's python bindings, the UI is Chainlit, the vector database is Chroma and everythin is glued together with Langchain. Document processing uses Spacy and Sentence Transformers and Playwright. There are no dependencies to external api's. Llama.cpp can use gpu acceleration with Cuda and Blas. See the documentation for llama-cpp-python for documentation. The chatbot uses character cards as prompts. The supported cards are Tavern and V2. Internal lorebooks are not supported yet. There are several scripts for parsing json lorebooks, pdt, textfiles and scarping web pages for the memory content. Also included are scripts for parsing metadata from documents automatically.","title":"llama-cpp-chat-memory"},{"location":"UNLICENSE/","text":"This is free and unencumbered software released into the public domain. Anyone is free to copy, modify, publish, use, compile, sell, or distribute this software, either in source code form or as a compiled binary, for any purpose, commercial or non-commercial, and by any means. In jurisdictions that recognize copyright laws, the author or authors of this software dedicate any and all copyright interest in the software to the public domain. We make this dedication for the benefit of the public at large and to the detriment of our heirs and successors. We intend this dedication to be an overt act of relinquishment in perpetuity of all present and future rights to this software under copyright law. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. For more information, please refer to https://unlicense.org","title":"License"},{"location":"card_format/","text":"Card Format See character editor . There are few example cards included like'Skynet', 'Harry Potter' and 'Bronya Zaychik' 'name' : 'char_name' The name for the ai character. When using json or yaml, this is expected to correspond to avatar image. name.png or name.jpg. 'description' : 'char_persona' The description for the character personality. Likes, dislikes, personality traits. 'scenario' : 'world_scenario' Description of the scenario. This roughly corresponds to things like \"You are a hr customer service having a discussion with a customer. Always be polite\". etc. 'mes_example' : 'example_dialogue' Example dialogue. The AI will pick answer patterns based on this 'first_mes' : 'char_greeting' A landing page for the chat. This will not be included in the prompt. The documents folder includes some documents for embeddings parsing for the character cards.","title":"Card Format"},{"location":"card_format/#card-format","text":"See character editor . There are few example cards included like'Skynet', 'Harry Potter' and 'Bronya Zaychik' 'name' : 'char_name' The name for the ai character. When using json or yaml, this is expected to correspond to avatar image. name.png or name.jpg. 'description' : 'char_persona' The description for the character personality. Likes, dislikes, personality traits. 'scenario' : 'world_scenario' Description of the scenario. This roughly corresponds to things like \"You are a hr customer service having a discussion with a customer. Always be polite\". etc. 'mes_example' : 'example_dialogue' Example dialogue. The AI will pick answer patterns based on this 'first_mes' : 'char_greeting' A landing page for the chat. This will not be included in the prompt. The documents folder includes some documents for embeddings parsing for the character cards.","title":"Card Format"},{"location":"configs/","text":"Configs You can change the configuration settings in .env file. The available embeddings are llama,spacy and hugginface. Make sure that the config for the chat matches the embeddings that were used to create the chroma collection. VECTOR_K is the value for vector storage documents for how many documents should be returned. You might need to change this based on your context and vector store chunk size. BUFFER_K is the size for conversation buffer. The prompt will include last K qustion answer pairs. Having large VECTOR_K and BUFFER_K can overfill the prompt. The default character card is Skynet_V2.png. This is just a basic template. Config Field Description MODEL_DIR The dir for the models MODEL model_name.gguf MODEL_TYPE alpaca/mistral LAYERS Number of layers to offload to gpu CHARACTER_CARD_DIR The directory for chracter cards CHARACTER_CARD character_card.png/yaml/json PERSIST_DIRECTORY dir for chroma embeddings PROMPT_TEMPLATE_DIRECTORY Prompt template are stored here PROMPT_TEMPLATE question_generation_template.json REPLACE_YOU Replace references to \"You\" in card with \"User\" KEY_STORAGE_DIRECTORY dir for NER keys for chroma USE_KEY_STORAGE Use NER keys for Chroma metadata COLLECTION Chroma collection to use. \"\" to disable Chroma QUERY_TYPE Embeddings type. \"mmr\" or \"similarity\" EMBEDDINGS_TYPE llama/spacy/hugginface FETCH_K Fetch k closest embeddings for similiarity LAMBDA_MULT Lambda for Chroma VECTOR_K Fetch k closest embeddings for mmr BUFFER_K Buffer last k exchanges to conversation context ROPE_CONTEXT Rope context for rope scaling N_CTX Context size USE_MAX_TOKENS Use max tokens. True/False MAX_TOKENS Max tokens","title":"Configs"},{"location":"configs/#configs","text":"You can change the configuration settings in .env file. The available embeddings are llama,spacy and hugginface. Make sure that the config for the chat matches the embeddings that were used to create the chroma collection. VECTOR_K is the value for vector storage documents for how many documents should be returned. You might need to change this based on your context and vector store chunk size. BUFFER_K is the size for conversation buffer. The prompt will include last K qustion answer pairs. Having large VECTOR_K and BUFFER_K can overfill the prompt. The default character card is Skynet_V2.png. This is just a basic template. Config Field Description MODEL_DIR The dir for the models MODEL model_name.gguf MODEL_TYPE alpaca/mistral LAYERS Number of layers to offload to gpu CHARACTER_CARD_DIR The directory for chracter cards CHARACTER_CARD character_card.png/yaml/json PERSIST_DIRECTORY dir for chroma embeddings PROMPT_TEMPLATE_DIRECTORY Prompt template are stored here PROMPT_TEMPLATE question_generation_template.json REPLACE_YOU Replace references to \"You\" in card with \"User\" KEY_STORAGE_DIRECTORY dir for NER keys for chroma USE_KEY_STORAGE Use NER keys for Chroma metadata COLLECTION Chroma collection to use. \"\" to disable Chroma QUERY_TYPE Embeddings type. \"mmr\" or \"similarity\" EMBEDDINGS_TYPE llama/spacy/hugginface FETCH_K Fetch k closest embeddings for similiarity LAMBDA_MULT Lambda for Chroma VECTOR_K Fetch k closest embeddings for mmr BUFFER_K Buffer last k exchanges to conversation context ROPE_CONTEXT Rope context for rope scaling N_CTX Context size USE_MAX_TOKENS Use max tokens. True/False MAX_TOKENS Max tokens","title":"Configs"},{"location":"creating_embeddings/","text":"Creating embeddings The embeddings creation uses env setting for threading and cuda. The Example documents are in the Documents folder. The scripts are in the documents_parsing folder. Use --help for basic instructions. The parsing script will parse all txt, pdf or json files in the target directory. For json lorebooks a key_storage file will also be created for metadata filtering. You need to download models for NER parsing. Textacy parses text files with Spacy sentence transformers to automatically generate keys for metadata filters. The default model is en_core_web_lg. See available models at Spacy Models python -m spacy download en_core_web_sm python -m spacy download en_core_web_md python -m spacy download en_core_web_lg You might want to play with the chunk size and overlap based on your text documents The example documents include a txt file for skynet embeddings and json lorebooks for Hogwarts and Honkai Impact The supported lorebook formats are chub inferred AgnAIstic and SillyTavern original source. For pdf files there is a pdf file of short stories from Fyodor Dostoyevsky included The source is Internet Archive, the copy is in public domain. The pdf text quality is quite poor thought, so I recommend getting another file, !!Important!!. You need to make sure that the documents, character_storage and key_storage folders exist. Textacy parsing will use NER to parse keys from the document using sentence transformers. This keys can be used as Chroma metadata, NOTE: Textacy parsing will create a key file in key_storage that can be used by text parsing. Json files will create keys automatically if present in json file. python -m document_parsing.textacy_parsing --collection-name skynet --embeddings-type spacy Parse the documents with python -m document_parsing.parse_text_documents --embeddings-type llama python -m document_parsing.parse_text_documents --collection-name skynet2 --embeddings-type spacy python -m document_parsing.parse_json_documents --embeddings-type spacy You can test the embeddings with python -m document_parsing.test_embeddings --collection-name skynet --query \"Who is John Connor\" --embeddings-type llama python -m document_parsing.test_embeddings --collection-name skynet2 --query \"Who is John Connor\" --embeddings-type spacy python -m document_parsing.test_embeddings --collection-name hogwarts --query \"Who is Charles Rookwood'\" --embeddings-type spacy","title":"Creating embeddings"},{"location":"creating_embeddings/#creating-embeddings","text":"The embeddings creation uses env setting for threading and cuda. The Example documents are in the Documents folder. The scripts are in the documents_parsing folder. Use --help for basic instructions. The parsing script will parse all txt, pdf or json files in the target directory. For json lorebooks a key_storage file will also be created for metadata filtering. You need to download models for NER parsing. Textacy parses text files with Spacy sentence transformers to automatically generate keys for metadata filters. The default model is en_core_web_lg. See available models at Spacy Models python -m spacy download en_core_web_sm python -m spacy download en_core_web_md python -m spacy download en_core_web_lg You might want to play with the chunk size and overlap based on your text documents The example documents include a txt file for skynet embeddings and json lorebooks for Hogwarts and Honkai Impact The supported lorebook formats are chub inferred AgnAIstic and SillyTavern original source. For pdf files there is a pdf file of short stories from Fyodor Dostoyevsky included The source is Internet Archive, the copy is in public domain. The pdf text quality is quite poor thought, so I recommend getting another file, !!Important!!. You need to make sure that the documents, character_storage and key_storage folders exist. Textacy parsing will use NER to parse keys from the document using sentence transformers. This keys can be used as Chroma metadata, NOTE: Textacy parsing will create a key file in key_storage that can be used by text parsing. Json files will create keys automatically if present in json file. python -m document_parsing.textacy_parsing --collection-name skynet --embeddings-type spacy Parse the documents with python -m document_parsing.parse_text_documents --embeddings-type llama python -m document_parsing.parse_text_documents --collection-name skynet2 --embeddings-type spacy python -m document_parsing.parse_json_documents --embeddings-type spacy You can test the embeddings with python -m document_parsing.test_embeddings --collection-name skynet --query \"Who is John Connor\" --embeddings-type llama python -m document_parsing.test_embeddings --collection-name skynet2 --query \"Who is John Connor\" --embeddings-type spacy python -m document_parsing.test_embeddings --collection-name hogwarts --query \"Who is Charles Rookwood'\" --embeddings-type spacy","title":"Creating embeddings"},{"location":"examples/","text":"Some examples","title":"Some Examples"},{"location":"examples/#some-examples","text":"","title":"Some examples"},{"location":"getting_started/","text":"You will need hatch to run this project. You can install hatch with pipx. See Hatch and Pipx . The commands here are for windows powershell. If you use another shell, you'll have to change things as needed. pip install pipx pipx install hatch Then from the repo root folder run. hatch shell chat cd .\\src\\llama_cpp_chat_memory\\ python -m spacy download en_core_web_lg playwright install You will need playwright for webscraping and the spacy models for text embeddings if you do not use llama-cpp embeddings. These are not needed for running the chatbot itself. You also might want to run llama-cpp with gpu acceleration like cuda. See llama-cpp-python for specifics. Then run: $env:FORCE_CMAKE=1 $env:CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --no-deps Note that this example is for powershell and for the latest llama-cpp-python. You will need to change the command based on the terminal and the llama-cpp-python version. Get a gguf model from a site like The Bloke and a character card and lorebooks from a site like Chub.ai or make your own with character editor Change the .env_test to .env and make sure that the correct folders exist. You can set the collection to \"\" and try the chatbot by running: chainlit run character_chat.py If you want to create memory then see more details below.","title":"Getting started"},{"location":"preparing_the_env/","text":"Preparing the env You will need a llama model that is compatible with llama-cpp. See models in HuggingFace by The Bloke You might want to build with cuda support. You need to pass FORCE_CMAKE=1 and CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" to env variables. This is the powershell syntax. Use whatever syntax your shell uses to set env variables You need to download language models if you use NER parsing, embeddings or spacy sentence transformers. The default model is en_core_web_lg. See available models at Spacy Models Choose the preferred model size and type. python -m spacy download en_core_web_sm python -m spacy download en_core_web_md python -m spacy download en_core_web_lg For installing dependencies in the virtual envs with hatch hatch env create Copy the .env_test to .env and set directories and model settings NOTE: Setting collection to \"\" will disable chroma fetching and you will get a normal character chatbot.","title":"Preparing the env"},{"location":"preparing_the_env/#preparing-the-env","text":"You will need a llama model that is compatible with llama-cpp. See models in HuggingFace by The Bloke You might want to build with cuda support. You need to pass FORCE_CMAKE=1 and CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" to env variables. This is the powershell syntax. Use whatever syntax your shell uses to set env variables You need to download language models if you use NER parsing, embeddings or spacy sentence transformers. The default model is en_core_web_lg. See available models at Spacy Models Choose the preferred model size and type. python -m spacy download en_core_web_sm python -m spacy download en_core_web_md python -m spacy download en_core_web_lg For installing dependencies in the virtual envs with hatch hatch env create Copy the .env_test to .env and set directories and model settings NOTE: Setting collection to \"\" will disable chroma fetching and you will get a normal character chatbot.","title":"Preparing the env"},{"location":"prompt_support/","text":"Prompt Support Supports alpaca and mistral text prompts, V2 and tavern style json and yaml files and V2 and tavern png cards. Avatar images need to be in the same folder as the prompt file. V2 and Tavern png files get a copy of the image without exif data in the project temp file. Inbuilt lorebooks are currently not supported See Chub.ai for some character cards or make your own with character editor .","title":"Prompt Support"},{"location":"prompt_support/#prompt-support","text":"Supports alpaca and mistral text prompts, V2 and tavern style json and yaml files and V2 and tavern png cards. Avatar images need to be in the same folder as the prompt file. V2 and Tavern png files get a copy of the image without exif data in the project temp file. Inbuilt lorebooks are currently not supported See Chub.ai for some character cards or make your own with character editor .","title":"Prompt Support"},{"location":"running_the_chatbot/","text":"Running the chatbot To run the chatbot. cd src\\llama_cpp_langchain_chat chainlit run character_chat.py The chatbot should open in your browser","title":"Running the chatbot"},{"location":"running_the_chatbot/#running-the-chatbot","text":"To run the chatbot. cd src\\llama_cpp_langchain_chat chainlit run character_chat.py The chatbot should open in your browser","title":"Running the chatbot"},{"location":"running_the_env/","text":"Running the env You'll need to run all the commands inside the virtual env. hatch shell chat (optional for cuda support)$env:FORCE_CMAKE=1 (optional for cuda support)$env:CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" (optional for cuda support)pip install llama-cpp-python==VERSION --force-reinstall --upgrade --no-cache-dir --no-deps cd src\\llama_cpp_langchain_chat","title":"Running the env"},{"location":"running_the_env/#running-the-env","text":"You'll need to run all the commands inside the virtual env. hatch shell chat (optional for cuda support)$env:FORCE_CMAKE=1 (optional for cuda support)$env:CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" (optional for cuda support)pip install llama-cpp-python==VERSION --force-reinstall --upgrade --no-cache-dir --no-deps cd src\\llama_cpp_langchain_chat","title":"Running the env"},{"location":"webscraping/","text":"Webscraping You can scrape web pages to text documents in order to use them as documents for chroma. The web scraping uses playwright and requires that the web engines are installed. After starting the virtual env run: playwright install The web scraping is prepared with config files in web_scrape_configs folder. The format is in json. See the example files for the specfics. The current impelemntation is unoptimized, so use with caution for a large number of pages. To run the scrape run: python -m document_parsing.web_scraper</BR> See --help for params","title":"Webscraping"},{"location":"webscraping/#webscraping","text":"You can scrape web pages to text documents in order to use them as documents for chroma. The web scraping uses playwright and requires that the web engines are installed. After starting the virtual env run: playwright install The web scraping is prepared with config files in web_scrape_configs folder. The format is in json. See the example files for the specfics. The current impelemntation is unoptimized, so use with caution for a large number of pages. To run the scrape run: python -m document_parsing.web_scraper</BR> See --help for params","title":"Webscraping"}]}