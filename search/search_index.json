{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"llama-cpp-chat-memory This project is intended as an example and a basic framework for a locally run chatbot with documents. The target user group is developers with some understanding about python and llm framworks. If you want to learn about llm and AI, when you can take a look at my llm resources for beginners or PygWiki . This project is mainly intended to serve as a more fleshed out tutorial and a basic frame to test various things like document embeddings. For this reason, the chatbot itself is intended to be lightweight and simple. You can also use this chatbot to test models and prompts. The document fetching can be disabled by setting collection to \"\" in the config files. This leaves you with just a basic character chatbot. Everything is designed to run locally. The model is run with llama.cpp and it's python bindings, the UI is Chainlit, the vector database is Chroma and everythin is glued together with Langchain. Document processing uses Spacy and Sentence Transformers and Playwright. There are no dependencies to external api's. Llama.cpp can use gpu acceleration with Cuda and Blas. See the documentation for llama-cpp-python for documentation. The chatbot uses character cards as prompts. The supported cards are Tavern and V2. Internal lorebooks are not supported yet. There are several scripts for parsing json lorebooks, pdt, textfiles and scarping web pages for the memory content. Also included are scripts for parsing metadata from documents automatically.","title":"Home"},{"location":"#llama-cpp-chat-memory","text":"This project is intended as an example and a basic framework for a locally run chatbot with documents. The target user group is developers with some understanding about python and llm framworks. If you want to learn about llm and AI, when you can take a look at my llm resources for beginners or PygWiki . This project is mainly intended to serve as a more fleshed out tutorial and a basic frame to test various things like document embeddings. For this reason, the chatbot itself is intended to be lightweight and simple. You can also use this chatbot to test models and prompts. The document fetching can be disabled by setting collection to \"\" in the config files. This leaves you with just a basic character chatbot. Everything is designed to run locally. The model is run with llama.cpp and it's python bindings, the UI is Chainlit, the vector database is Chroma and everythin is glued together with Langchain. Document processing uses Spacy and Sentence Transformers and Playwright. There are no dependencies to external api's. Llama.cpp can use gpu acceleration with Cuda and Blas. See the documentation for llama-cpp-python for documentation. The chatbot uses character cards as prompts. The supported cards are Tavern and V2. Internal lorebooks are not supported yet. There are several scripts for parsing json lorebooks, pdt, textfiles and scarping web pages for the memory content. Also included are scripts for parsing metadata from documents automatically.","title":"llama-cpp-chat-memory"},{"location":"UNLICENSE/","text":"This is free and unencumbered software released into the public domain. Anyone is free to copy, modify, publish, use, compile, sell, or distribute this software, either in source code form or as a compiled binary, for any purpose, commercial or non-commercial, and by any means. In jurisdictions that recognize copyright laws, the author or authors of this software dedicate any and all copyright interest in the software to the public domain. We make this dedication for the benefit of the public at large and to the detriment of our heirs and successors. We intend this dedication to be an overt act of relinquishment in perpetuity of all present and future rights to this software under copyright law. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. For more information, please refer to https://unlicense.org","title":"License"},{"location":"card_format/","text":"Card Format See character editor . There are few example cards included like'Skynet' and 'Shodan' 'name' : 'char_name' The name for the ai character. When using json or yaml, this is expected to correspond to avatar image. name.png or name.jpg. 'description' : 'char_persona' The description for the character personality. Likes, dislikes, personality traits. 'scenario' : 'world_scenario' Description of the scenario. This roughly corresponds to things like \"You are a hr customer service having a discussion with a customer. Always be polite\". etc. 'mes_example' : 'example_dialogue' Example dialogue. The AI will pick answer patterns based on this 'first_mes' : 'char_greeting' A landing page for the chat. This will not be included in the prompt. The documents folder includes some documents for embeddings parsing for the character cards.","title":"Card Format"},{"location":"card_format/#card-format","text":"See character editor . There are few example cards included like'Skynet' and 'Shodan' 'name' : 'char_name' The name for the ai character. When using json or yaml, this is expected to correspond to avatar image. name.png or name.jpg. 'description' : 'char_persona' The description for the character personality. Likes, dislikes, personality traits. 'scenario' : 'world_scenario' Description of the scenario. This roughly corresponds to things like \"You are a hr customer service having a discussion with a customer. Always be polite\". etc. 'mes_example' : 'example_dialogue' Example dialogue. The AI will pick answer patterns based on this 'first_mes' : 'char_greeting' A landing page for the chat. This will not be included in the prompt. The documents folder includes some documents for embeddings parsing for the character cards.","title":"Card Format"},{"location":"configs/","text":"Basic Configs You can change the configuration settings in .env file. The available embeddings are llama,spacy and hugginface. Make sure that the config for the chat matches the embeddings that were used to create the chroma collection. VECTOR_K is the value for vector storage documents for how many documents should be returned. You might need to change this based on your context and vector store chunk size. BUFFER_K is the size for conversation buffer. The prompt will include last K qustion answer pairs. Having large VECTOR_K and BUFFER_K can overfill the prompt. The default character card is Skynet_V2.png. This is just a basic template. Config Field Description MODEL_DIR The dir for the models MODEL model_name.gguf MODEL_TYPE alpaca/mistral CHARACTER_CARD_DIR The directory for chracter cards CHARACTER_CARD character_card.png/yaml/json PERSIST_DIRECTORY dir for chroma embeddings PROMPT_TEMPLATE_DIRECTORY Prompt template are stored here REPLACE_YOU Replace references to \"You\" in card with \"User\" KEY_STORAGE_DIRECTORY dir for NER keys for chroma COLLECTION_CONFIG Path to run config file for collection and prompt EMBEDDINGS_TYPE llama/spacy/hugginface EMBEDDINGS_MODEL spacy/hugginface model name (needs to be installed) CUSTOM_CSS Url to the custom css file to be used by the application. VECTOR_K Fetch k closest embeddings for mmr BUFFER_K Buffer last k exchanges to conversation context FETCH_K Fetch k closest embeddings for similiarity LAMBDA_MULT Lambda for Chroma LAYERS Number of layers to offload to gpu SEED Seed used for generation. Default random (-1) N_PARTS How many parts the model is divided into. Default auto (-1) USE_MLOCK Load the whole model into ram. Default False USE_MMAP Allows only the necessary parts to be loaded into memory and offloading the rest. Default false TEMPERATURE Adjust the randomness of the generated text (default: 0.8) TOP_P A higher value for top-p (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. The default value is 0.9. TOP_K A higher value for top-k (e.g., 100) will consider more tokens and lead to more diverse text, while a lower value (e.g., 10) will focus on the most probable tokens and generate more conservative text. The default value is 40. REPEAT_PENALTY The repeat-penalty option helps prevent the model from generating repetitive or monotonous text. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. The default value is 1.1. LAST_N_TOKENS_SIZE Last n tokens to consider for penalizing repetition VERBOSE Verbose mode. Default True ROPE_CONTEXT Rope context for rope scaling N_CTX Context size N_BATCH Message write batch size MAX_TOKENS Max tokens. Default 256 General Configs Other configs are found in the run_files folder. These include Webscrape configs, ner parse configs and filter configs. Filters folder defines the general webscrape filters to clean the documents. This file uses regex and can easily be modified to add extra filtering. Parse_configs defines the expected csv column structure and ner type parsing. This includes noun engrams, entities, noun chunks and parse type. Web scrape configs define the web pages fo a scrape. This is convinient if you want to scrape multiple pages. Run Config The run config in run_config.json in the run_files folder defines the options for chat run settings. The run config sets the defaults for message collection, context collection and for the prompt template. The config also gives the list of alternative collection and prompt settings. These can be changed while the chat is running from the chat settings menu.","title":"Configs"},{"location":"configs/#basic-configs","text":"You can change the configuration settings in .env file. The available embeddings are llama,spacy and hugginface. Make sure that the config for the chat matches the embeddings that were used to create the chroma collection. VECTOR_K is the value for vector storage documents for how many documents should be returned. You might need to change this based on your context and vector store chunk size. BUFFER_K is the size for conversation buffer. The prompt will include last K qustion answer pairs. Having large VECTOR_K and BUFFER_K can overfill the prompt. The default character card is Skynet_V2.png. This is just a basic template. Config Field Description MODEL_DIR The dir for the models MODEL model_name.gguf MODEL_TYPE alpaca/mistral CHARACTER_CARD_DIR The directory for chracter cards CHARACTER_CARD character_card.png/yaml/json PERSIST_DIRECTORY dir for chroma embeddings PROMPT_TEMPLATE_DIRECTORY Prompt template are stored here REPLACE_YOU Replace references to \"You\" in card with \"User\" KEY_STORAGE_DIRECTORY dir for NER keys for chroma COLLECTION_CONFIG Path to run config file for collection and prompt EMBEDDINGS_TYPE llama/spacy/hugginface EMBEDDINGS_MODEL spacy/hugginface model name (needs to be installed) CUSTOM_CSS Url to the custom css file to be used by the application. VECTOR_K Fetch k closest embeddings for mmr BUFFER_K Buffer last k exchanges to conversation context FETCH_K Fetch k closest embeddings for similiarity LAMBDA_MULT Lambda for Chroma LAYERS Number of layers to offload to gpu SEED Seed used for generation. Default random (-1) N_PARTS How many parts the model is divided into. Default auto (-1) USE_MLOCK Load the whole model into ram. Default False USE_MMAP Allows only the necessary parts to be loaded into memory and offloading the rest. Default false TEMPERATURE Adjust the randomness of the generated text (default: 0.8) TOP_P A higher value for top-p (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. The default value is 0.9. TOP_K A higher value for top-k (e.g., 100) will consider more tokens and lead to more diverse text, while a lower value (e.g., 10) will focus on the most probable tokens and generate more conservative text. The default value is 40. REPEAT_PENALTY The repeat-penalty option helps prevent the model from generating repetitive or monotonous text. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. The default value is 1.1. LAST_N_TOKENS_SIZE Last n tokens to consider for penalizing repetition VERBOSE Verbose mode. Default True ROPE_CONTEXT Rope context for rope scaling N_CTX Context size N_BATCH Message write batch size MAX_TOKENS Max tokens. Default 256","title":"Basic Configs"},{"location":"configs/#general-configs","text":"Other configs are found in the run_files folder. These include Webscrape configs, ner parse configs and filter configs. Filters folder defines the general webscrape filters to clean the documents. This file uses regex and can easily be modified to add extra filtering. Parse_configs defines the expected csv column structure and ner type parsing. This includes noun engrams, entities, noun chunks and parse type. Web scrape configs define the web pages fo a scrape. This is convinient if you want to scrape multiple pages.","title":"General Configs"},{"location":"configs/#run-config","text":"The run config in run_config.json in the run_files folder defines the options for chat run settings. The run config sets the defaults for message collection, context collection and for the prompt template. The config also gives the list of alternative collection and prompt settings. These can be changed while the chat is running from the chat settings menu.","title":"Run Config"},{"location":"creating_embeddings/","text":"Creating embeddings The embeddings creation uses env setting for threading and cuda. The Example documents are in the Documents folder. The scripts are in the documents_parsing folder. Use --help for basic instructions. The parsing script will parse all txt, pdf or json files in the target directory. For json lorebooks a key_storage file will also be created for metadata filtering. You need to download models for NER parsing. Textacy parses text files with Spacy sentence transformers to automatically generate keys for metadata filters. The default model is en_core_web_lg. See available models at Spacy Models python -m spacy download en_core_web_sm python -m spacy download en_core_web_md python -m spacy download en_core_web_lg You might want to play with the chunk size and overlap based on your text documents The example documents include a txt file for 'Skynet' and 'Shodan' The supported lorebook formats are chub inferred AgnAIstic and SillyTavern original source. For pdf files there is a pdf file of short stories from Fyodor Dostoyevsky included The source is Internet Archive, the copy is in public domain. The pdf text quality is quite poor thought, so I recommend getting another file. Performance for files over 200mb is not great. Parsing a large text with a large keyfile will result in poor performance. It's more effective to have smaller colletions that have their own keyfiles rather that one large collection with one keyfile. I recommend splitting sollections my subject category and then switching as needed. !!Important!!. You need to make sure that the documents, character_storage and key_storage folders exist. Textacy parsing will use NER to parse keys from the document using sentence transformers. This keys can be used as Chroma metadata, NOTE: Textacy parsing will create a key file in key_storage that can be used by text parsing. Json files will create keys automatically if present in json file. python -m document_parsing.textacy_parsing --collection-name skynet --embeddings-type spacy Parse csv to text python -m document_parsing.parse_csv_to_text Parse the documents with. The new document parsing uses multiprocess to parse metadata keys created with parse_ner script. This increases the processing speed with large key files by a significant margin. The old script uses a single thread for processing keys and this can cause significant slowdown with many documents with large keyfiles. You can give the number of threads for the multiprocess with --threads python -m document_parsing.parse_text_documents python -m document_parsing.parse_text_documents python -m document_parsing.parse_json_documents You can test the embeddings with python -m document_parsing.test_embeddings --collection-name skynet --query \"Who is John Connor\" --embeddings-type llama python -m document_parsing.test_embeddings --collection-name skynet2 --query \"Who is John Connor\" --embeddings-type spacy python -m document_parsing.test_embeddings --collection-name hogwarts --query \"Who is Charles Rookwood'\" --embeddings-type spacy Optional params Description --data-directory The directory where your text files are stored. Default \"./run_files/documents/skynet\" --collection-name The name of the collection. Default \"skynet\" --persist-directory The directory where you want to store the Chroma collection. Default \"./run_files/character_storage/\" --key-storage The directory for the collection metadata keys Need to be created with textacy parsing. Default \"./run_files/key_storage/\" --chunk-size The text chunk size for parsing. Default \"1024\" --chunk-overlap The overlap for text chunks for parsing. Default \"0\" --embeddings-type The chosen embeddings type. Default \"spacy\"","title":"Creating embeddings"},{"location":"creating_embeddings/#creating-embeddings","text":"The embeddings creation uses env setting for threading and cuda. The Example documents are in the Documents folder. The scripts are in the documents_parsing folder. Use --help for basic instructions. The parsing script will parse all txt, pdf or json files in the target directory. For json lorebooks a key_storage file will also be created for metadata filtering. You need to download models for NER parsing. Textacy parses text files with Spacy sentence transformers to automatically generate keys for metadata filters. The default model is en_core_web_lg. See available models at Spacy Models python -m spacy download en_core_web_sm python -m spacy download en_core_web_md python -m spacy download en_core_web_lg You might want to play with the chunk size and overlap based on your text documents The example documents include a txt file for 'Skynet' and 'Shodan' The supported lorebook formats are chub inferred AgnAIstic and SillyTavern original source. For pdf files there is a pdf file of short stories from Fyodor Dostoyevsky included The source is Internet Archive, the copy is in public domain. The pdf text quality is quite poor thought, so I recommend getting another file. Performance for files over 200mb is not great. Parsing a large text with a large keyfile will result in poor performance. It's more effective to have smaller colletions that have their own keyfiles rather that one large collection with one keyfile. I recommend splitting sollections my subject category and then switching as needed. !!Important!!. You need to make sure that the documents, character_storage and key_storage folders exist. Textacy parsing will use NER to parse keys from the document using sentence transformers. This keys can be used as Chroma metadata, NOTE: Textacy parsing will create a key file in key_storage that can be used by text parsing. Json files will create keys automatically if present in json file. python -m document_parsing.textacy_parsing --collection-name skynet --embeddings-type spacy Parse csv to text python -m document_parsing.parse_csv_to_text Parse the documents with. The new document parsing uses multiprocess to parse metadata keys created with parse_ner script. This increases the processing speed with large key files by a significant margin. The old script uses a single thread for processing keys and this can cause significant slowdown with many documents with large keyfiles. You can give the number of threads for the multiprocess with --threads python -m document_parsing.parse_text_documents python -m document_parsing.parse_text_documents python -m document_parsing.parse_json_documents You can test the embeddings with python -m document_parsing.test_embeddings --collection-name skynet --query \"Who is John Connor\" --embeddings-type llama python -m document_parsing.test_embeddings --collection-name skynet2 --query \"Who is John Connor\" --embeddings-type spacy python -m document_parsing.test_embeddings --collection-name hogwarts --query \"Who is Charles Rookwood'\" --embeddings-type spacy Optional params Description --data-directory The directory where your text files are stored. Default \"./run_files/documents/skynet\" --collection-name The name of the collection. Default \"skynet\" --persist-directory The directory where you want to store the Chroma collection. Default \"./run_files/character_storage/\" --key-storage The directory for the collection metadata keys Need to be created with textacy parsing. Default \"./run_files/key_storage/\" --chunk-size The text chunk size for parsing. Default \"1024\" --chunk-overlap The overlap for text chunks for parsing. Default \"0\" --embeddings-type The chosen embeddings type. Default \"spacy\"","title":"Creating embeddings"},{"location":"csv/","text":"Named Entity Recognition(NER) You can use filter_csv.py and parse_csv_to_text.py to process csv files. The filter script will remove rows using whitelists and blacklists. You can set a filter for any column. This is useful when you want to split large documents to more manageable portions. The csv to text parsing document filters web elements if you have webscraped data into csv.","title":"Csv filtering and parsing"},{"location":"csv/#named-entity-recognitionner","text":"You can use filter_csv.py and parse_csv_to_text.py to process csv files. The filter script will remove rows using whitelists and blacklists. You can set a filter for any column. This is useful when you want to split large documents to more manageable portions. The csv to text parsing document filters web elements if you have webscraped data into csv.","title":"Named Entity Recognition(NER)"},{"location":"examples/","text":"Some examples","title":"Some Examples"},{"location":"examples/#some-examples","text":"","title":"Some examples"},{"location":"getting_started/","text":"You will need hatch to run this project. You can install hatch with pipx. See Hatch and Pipx . The commands here are for windows powershell. If you use another shell, you'll have to change things as needed. pip install pipx pipx install hatch Then from the repo root folder run. hatch shell chat cd .\\src\\llama_cpp_chat_memory\\ python -m spacy download en_core_web_lg playwright install You will need spacy models for text embeddings if you do not use llama-cpp embeddings. Playwright is used by the old webscrape scripts. These are not needed for running the chatbot itself. You also might want to run llama-cpp with gpu acceleration like cuda. See llama-cpp-python for specifics. Then run: $env:FORCE_CMAKE=1 $env:CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --no-deps Note that this example is for powershell and for the latest llama-cpp-python. You will need to change the command based on the terminal and the llama-cpp-python version. Get a gguf model from a site like The Bloke and a character card and lorebooks from a site like Character hub or make your own with character editor Change the .env_test to .env and make sure that the correct folders exist. You can set the collection to \"\" and try the chatbot by running: chainlit run character_chat.py If you want to create memory then see more details below.","title":"Getting started"},{"location":"named_entity_recognition/","text":"Named Entity Recognition(NER) You can use textacy_parsing script for generating document metadata keys automatically. The scripts are a modified version of textacy code updated to run with the current spacy version. The script uses a spacy embeddings model to process a text document for a json metadata keyfile. The keys are parsed based on a config file in run_files/parse_configs/ner_types.json or run_files/parse_configs/ner_types_full.json. You can give your own config file if you want. The new parse script uses multiprocess to improve performance. The default process pool number is 6. You should change the process number based on the number of cores your machine has. The available configs are Ngrams Description PROPN Proper Noun NOUN Noun ADJ Adjective NNP Noun proper singular NN Noun, singular or mass AUX Auxiliary VBZ Verb, 3rd person singular present VERB Verb ADP Adposition SYM Symbol NUM Numeral CD Cardinal number VBG verb, gerund or present participle ROOT Root Entities Description FAC Buildings, airports, highways, bridges, etc. NORP Nationalities or religious or political groups GPE Countries, cities, states PRODUCT Objects, vehicles, foods, etc. (not services) EVENT Named hurricanes, battles, wars, sports events, etc. PERSON People, including fictional ORG Companies, agencies, institutions, etc. LOC Non-GPE locations, mountain ranges, bodies of water DATE Absolute or relative dates or periods TIME Times smaller than a day WORK_OF_ART Titles of books, songs, etc. Extract type Description orth Terms are represented by their text exactly as written lower Lowercased form of the text lemma Base form w/o inflectional suffixes For details see Spacy linguistic features and Model NER labels . The instructions expect en model, but spacy supports a wide range of models. You can also specify Noun chunks. Noun chunk of 2 for example would create keys like \"Yellow House\" or \"Blond Hair\". You can create ner metadata list with python -m document_parsing.parse_ner Optional param Description --data-directory The directory where your text files are stored. Default \"./run_files/documents/skynet\" --collection-name The name of the collection Will be used as name and location for the keyfile. Default \"skynet\" --key-storage The directory for the collection metadata keys. Default \"./run_files/key_storage/\" --threads The number of multiprocess threads. Default 6.","title":"Named Entity Recognition(NER)"},{"location":"named_entity_recognition/#named-entity-recognitionner","text":"You can use textacy_parsing script for generating document metadata keys automatically. The scripts are a modified version of textacy code updated to run with the current spacy version. The script uses a spacy embeddings model to process a text document for a json metadata keyfile. The keys are parsed based on a config file in run_files/parse_configs/ner_types.json or run_files/parse_configs/ner_types_full.json. You can give your own config file if you want. The new parse script uses multiprocess to improve performance. The default process pool number is 6. You should change the process number based on the number of cores your machine has. The available configs are Ngrams Description PROPN Proper Noun NOUN Noun ADJ Adjective NNP Noun proper singular NN Noun, singular or mass AUX Auxiliary VBZ Verb, 3rd person singular present VERB Verb ADP Adposition SYM Symbol NUM Numeral CD Cardinal number VBG verb, gerund or present participle ROOT Root Entities Description FAC Buildings, airports, highways, bridges, etc. NORP Nationalities or religious or political groups GPE Countries, cities, states PRODUCT Objects, vehicles, foods, etc. (not services) EVENT Named hurricanes, battles, wars, sports events, etc. PERSON People, including fictional ORG Companies, agencies, institutions, etc. LOC Non-GPE locations, mountain ranges, bodies of water DATE Absolute or relative dates or periods TIME Times smaller than a day WORK_OF_ART Titles of books, songs, etc. Extract type Description orth Terms are represented by their text exactly as written lower Lowercased form of the text lemma Base form w/o inflectional suffixes For details see Spacy linguistic features and Model NER labels . The instructions expect en model, but spacy supports a wide range of models. You can also specify Noun chunks. Noun chunk of 2 for example would create keys like \"Yellow House\" or \"Blond Hair\". You can create ner metadata list with python -m document_parsing.parse_ner Optional param Description --data-directory The directory where your text files are stored. Default \"./run_files/documents/skynet\" --collection-name The name of the collection Will be used as name and location for the keyfile. Default \"skynet\" --key-storage The directory for the collection metadata keys. Default \"./run_files/key_storage/\" --threads The number of multiprocess threads. Default 6.","title":"Named Entity Recognition(NER)"},{"location":"preparing_the_env/","text":"Preparing the env You will need a llama model that is compatible with llama-cpp. See models in HuggingFace by The Bloke You might want to build with cuda support. You need to pass FORCE_CMAKE=1 and CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" to env variables. This is the powershell syntax. Use whatever syntax your shell uses to set env variables You need to download language models if you use NER parsing, embeddings or spacy sentence transformers. The default model is en_core_web_lg. See available models at Spacy Models Choose the preferred model size and type. python -m spacy download en_core_web_sm python -m spacy download en_core_web_md python -m spacy download en_core_web_lg For installing dependencies in the virtual envs with hatch hatch env create Copy the .env_test to .env and set directories and model settings NOTE: Setting collection to \"\" will disable chroma fetching and you will get a normal character chatbot.","title":"Preparing the env"},{"location":"preparing_the_env/#preparing-the-env","text":"You will need a llama model that is compatible with llama-cpp. See models in HuggingFace by The Bloke You might want to build with cuda support. You need to pass FORCE_CMAKE=1 and CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" to env variables. This is the powershell syntax. Use whatever syntax your shell uses to set env variables You need to download language models if you use NER parsing, embeddings or spacy sentence transformers. The default model is en_core_web_lg. See available models at Spacy Models Choose the preferred model size and type. python -m spacy download en_core_web_sm python -m spacy download en_core_web_md python -m spacy download en_core_web_lg For installing dependencies in the virtual envs with hatch hatch env create Copy the .env_test to .env and set directories and model settings NOTE: Setting collection to \"\" will disable chroma fetching and you will get a normal character chatbot.","title":"Preparing the env"},{"location":"prompt_support/","text":"Prompt Support Supports alpaca and mistral text prompts, V2 and tavern style json and yaml files and V2 and tavern png cards. Avatar images need to be in the same folder as the prompt file. V2 and Tavern png files get a copy of the image without exif data in the project temp file. Inbuilt lorebooks are currently not supported See Character hub for some character cards or make your own with character editor .","title":"Prompt Support"},{"location":"prompt_support/#prompt-support","text":"Supports alpaca and mistral text prompts, V2 and tavern style json and yaml files and V2 and tavern png cards. Avatar images need to be in the same folder as the prompt file. V2 and Tavern png files get a copy of the image without exif data in the project temp file. Inbuilt lorebooks are currently not supported See Character hub for some character cards or make your own with character editor .","title":"Prompt Support"},{"location":"running_the_chatbot/","text":"Running the chatbot To run the chatbot. You need to run the chat with the custom script instead of the chainlit run command. The reason for this is the updates for the config files when switching character. These changes need to be done before calling chainlit. If you call chainlit directly, the character name and avatar picture won't update. Note: Currently something seems to be cached by chainlit. Until I find a way to clear the cache, you need to call run_chat twice for changes to take effect. Some browsers don't allow loading css file from local directories. For testing purposes there is a flask script to run a simple http server that serves stylesheets from the \"static/\" directory. You will need to run the flask server in another terminal instance. cd src\\llama_cpp_langchain_chat python -m run_chat The chatbot should open in your browser Running flask hatch shell chat cd .\\src\\llama_cpp_chat_memory\\ flask --app flask_web_server run","title":"Running the chatbot"},{"location":"running_the_chatbot/#running-the-chatbot","text":"To run the chatbot. You need to run the chat with the custom script instead of the chainlit run command. The reason for this is the updates for the config files when switching character. These changes need to be done before calling chainlit. If you call chainlit directly, the character name and avatar picture won't update. Note: Currently something seems to be cached by chainlit. Until I find a way to clear the cache, you need to call run_chat twice for changes to take effect. Some browsers don't allow loading css file from local directories. For testing purposes there is a flask script to run a simple http server that serves stylesheets from the \"static/\" directory. You will need to run the flask server in another terminal instance. cd src\\llama_cpp_langchain_chat python -m run_chat The chatbot should open in your browser Running flask hatch shell chat cd .\\src\\llama_cpp_chat_memory\\ flask --app flask_web_server run","title":"Running the chatbot"},{"location":"running_the_env/","text":"Running the env You'll need to run all the commands inside the virtual env. Some browsers don't allow loading css file from local directories. For testing purposes there is a flask script to run a simple http server that serves stylesheets from the \"static/\" directory. You will need to run the flask server in another terminal instance. hatch shell chat (optional for cuda support)$env:FORCE_CMAKE=1 (optional for cuda support)$env:CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" (optional for cuda support)pip install llama-cpp-python==VERSION --force-reinstall --upgrade --no-cache-dir --no-deps cd src\\llama_cpp_langchain_chat Running flask hatch shell chat cd .\\src\\llama_cpp_chat_memory\\ flask --app flask_web_server run","title":"Running the env"},{"location":"running_the_env/#running-the-env","text":"You'll need to run all the commands inside the virtual env. Some browsers don't allow loading css file from local directories. For testing purposes there is a flask script to run a simple http server that serves stylesheets from the \"static/\" directory. You will need to run the flask server in another terminal instance. hatch shell chat (optional for cuda support)$env:FORCE_CMAKE=1 (optional for cuda support)$env:CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" (optional for cuda support)pip install llama-cpp-python==VERSION --force-reinstall --upgrade --no-cache-dir --no-deps cd src\\llama_cpp_langchain_chat Running flask hatch shell chat cd .\\src\\llama_cpp_chat_memory\\ flask --app flask_web_server run","title":"Running the env"},{"location":"webscraping/","text":"Webscraping You can scrape web pages to text documents in order to use them as documents for chroma. Optional. The old web scraping uses playwright and requires that the web engines are installed. After starting the virtual env run: playwright install The web scraping is prepared with config files in web_scrape_configs folder. The format is in json. See the example files for the specfics. A number of regex filters are used to clean the scrape data. You can modify and add filters if you want. The filters are stored in the src/llama_cpp_chat_memory/run_files/filters/web_scrape_filters.json file. To run the scrape run: python -m document_parsing.web_scraper</BR> Optional param Description --data-directory The directory where your text files are stored. Default \"./run_files/documents/skynet\" --collection-name The name of the collection. Default \"skynet\" --web-scrape-directory The config file to be used for the webscrape. Default \"./run_files/web_scrape_configs/\"","title":"Webscraping"},{"location":"webscraping/#webscraping","text":"You can scrape web pages to text documents in order to use them as documents for chroma. Optional. The old web scraping uses playwright and requires that the web engines are installed. After starting the virtual env run: playwright install The web scraping is prepared with config files in web_scrape_configs folder. The format is in json. See the example files for the specfics. A number of regex filters are used to clean the scrape data. You can modify and add filters if you want. The filters are stored in the src/llama_cpp_chat_memory/run_files/filters/web_scrape_filters.json file. To run the scrape run: python -m document_parsing.web_scraper</BR> Optional param Description --data-directory The directory where your text files are stored. Default \"./run_files/documents/skynet\" --collection-name The name of the collection. Default \"skynet\" --web-scrape-directory The config file to be used for the webscrape. Default \"./run_files/web_scrape_configs/\"","title":"Webscraping"}]}