[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "llama-cpp-chat-memory"
dynamic = ["version"]
description = 'llama_cpp chat with langhcain, chainlit and vectorstore memory.'
readme = "README.md"
license = "UNLICENSE"
keywords = []
authors = [
  { name = "ossirytk", email = "ossirytk@gmail.com" },
]
classifiers = [
  "Development Status :: 4 - Beta",
  "Programming Language :: Python",
  "Programming Language :: Python :: 3.11",
]
dependencies = [
"langchain==0.2.0",
"langchain-community==0.2.0",
"chainlit==1.1.101",
"llama-cpp-python==0.2.75",
"Pillow==10.3.0",
"PyYAML==6.0.1",
"toml==0.10.2",
"chromadb==0.5.0",
"pypdf==4.2.0",
"sentence-transformers==2.7.0",
"simsimd==4.3.1",
"beautifulsoup4==4.12.3",
"playwright==1.44.0",
"pydantic==2.7.1",
"cytoolz==0.12.3",
"spacy==3.7.4",
"pandas==2.2.2",
"pyarrow==16.1.0",
"trafilatura==1.9.0",
"flask==3.0.3",
]

[project.urls]
Documentation = "https://github.com/ossirytk/llama-cpp-chat-memory/blob/main/README.md"
Issues = "https://github.com/ossirytk/llama-cpp-chat-memory/issues"
Source = "https://github.com/ossirytk/llama-cpp-chat-memory"

[tool.hatch.version]
path = "src/llama_cpp_chat_memory/__about__.py"

[tool.hatch.envs.chat]
decription="Llama cpp chat with vector store memory"
dependencies = [
  "coverage[toml]>=6.5",
  "pytest",
]
[tool.hatch.envs.chat.scripts]
test = "pytest {args:tests}"
test-cov = "coverage run -m pytest {args:tests}"
cov-report = [
  "- coverage combine",
  "coverage report",
]
cov = [
  "test-cov",
  "cov-report",
]

[[tool.hatch.envs.all.matrix]]
python = ["3.11"]

[tool.hatch.envs.lint]
detached = true
dependencies = [
  "black>=23.1.0",
  "mypy>=1.0.0",
  "ruff>=0.0.243",
]
[tool.hatch.envs.lint.scripts]
typing = "mypy --install-types --non-interactive {args:src/llama_cpp_chat_memory tests}"
style = [
  "ruff {args:.}",
  "black --check --diff {args:.}",
]
fmt = [
  "black {args:.}",
  "ruff --fix {args:.}",
  "style",
]
all = [
  "style",
  "typing",
]

[tool.black]
target-version = ["py311"]
line-length = 120
skip-string-normalization = true

[tool.ruff]
target-version = "py311"
line-length = 120
select = [
  "A",
  "ARG",
  "B",
  "C",
  "DTZ",
  "E",
  "EM",
  "F",
  "FBT",
  "I",
  "ICN",
  "ISC",
  "N",
  "PLC",
  "PLE",
  "PLR",
  "PLW",
  "Q",
  "RUF",
  "S",
  "T",
  "TID",
  "UP",
  "W",
  "YTT",
]
ignore = [
  # Allow non-abstract empty methods in abstract base classes
  "B027",
  # Allow boolean positional values in function calls, like `dict.get(... True)`
  "FBT003",
  # Ignore checks for possible passwords
  "S105", "S106", "S107",
  # Ignore complexity
  "C901", "PLR0911", "PLR0912", "PLR0913", "PLR0915",
]
unfixable = [
  # Don't touch unused imports
  "F401",
]

[tool.ruff.isort]
known-first-party = ["llama_cpp_chat_memory"]

[tool.ruff.flake8-tidy-imports]
ban-relative-imports = "all"

[tool.ruff.per-file-ignores]
# Tests can use magic values, assertions, and relative imports
"tests/**/*" = ["PLR2004", "S101", "TID252"]

[tool.coverage.run]
source_pkgs = ["llama_cpp_chat_memory", "tests"]
branch = true
parallel = true
omit = [
  "src/llama_cpp_chat_memory/__about__.py",
]

[tool.coverage.paths]
llama_cpp_chat_memory = ["src/llama_cpp_chat_memory", "*/llama-cpp-chat-memory/src/llama_cpp_chat_memory"]
tests = ["tests", "*/llama-cpp-chat-memory/tests"]

[tool.coverage.report]
exclude_lines = [
  "no cov",
  "if __name__ == .__main__.:",
  "if TYPE_CHECKING:",
]
